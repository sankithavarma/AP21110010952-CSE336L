{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3587b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff6ced2",
   "metadata": {},
   "source": [
    "# 1. Implement Linear Regression and calculate sum of residual error on the following\n",
    "Datasets.\n",
    "x = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "y = [1, 3, 2, 5, 7, 8, 8, 9, 10, 12]\n",
    " Compute the regression coefficients using analytic formulation and calculate Sum\n",
    "Squared Error (SSE) and R 2 value.\n",
    " Implement gradient descent (both Full-batch and Stochastic with stopping\n",
    "criteria) on Least Mean Square loss formulation to compute the coefficients of\n",
    "regression matrix and compare the results using performance measures such as R 2\n",
    "SSE etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ec5b204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression using analytic formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4092ed12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.23636364 1.16969697]\n",
      "5.624242424242421\n",
      "0.952538038613988\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])\n",
    "X = np.vstack([np.ones_like(x), x]).T\n",
    "coefficients = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "y_pred = X.dot(coefficients)\n",
    "SSE = np.sum((y - y_pred) ** 2)\n",
    "y_mean = np.mean(y)\n",
    "SST = np.sum((y - y_mean) ** 2)\n",
    "Rsquare = 1 - (SSE / SST)\n",
    "print(coefficients)\n",
    "print(SSE)\n",
    "print(Rsquare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebed36bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytic Solution: \n",
      "b0: 1.2363636363636372\n",
      "b1: 1.1696969696969695\n",
      "SSE: 5.624242424242422\n",
      "R^2: 0.952538038613988\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])\n",
    "def linear_regression_analytic(x, y):\n",
    "    n = len(x)\n",
    "    x1 = np.mean(x)\n",
    "    y1 = np.mean(y)\n",
    "    xy = np.mean(x*y)\n",
    "    xx1 = np.mean(x**2)\n",
    "\n",
    "    # Compute regression coefficients\n",
    "    b1 = (xy - x1*y1) / (xx1 - x1**2)\n",
    "    b0 = y1 - b1*x1\n",
    "\n",
    "    # Compute predicted values\n",
    "    y_pred = b0 + b1*x\n",
    "\n",
    "    # Compute Sum of Squared Error (SSE)\n",
    "    sse = np.sum((y - y_pred)**2)\n",
    "\n",
    "    # Compute R^2 value\n",
    "    sst = np.sum((y - y1)**2)\n",
    "    r_squared = 1 - (sse / sst)\n",
    "\n",
    "    return b0, b1, sse, r_squared\n",
    "b0,b1,sse,r_sq=linear_regression_analytic(x,y)\n",
    "print(\"Analytic Solution: \")\n",
    "print(\"b0:\",b0)\n",
    "print(\"b1:\",b1)\n",
    "print(\"SSE:\",sse)\n",
    "print(\"R^2:\",r_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0b7b29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression using full-batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36fff7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full-batch Gradient Descent:\n",
      "b0: 1.230898466943318\n",
      "b1: 1.170568526128318\n",
      "SSE: 5.624329890820989\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])\n",
    "def full_batch_gradient_descent(x, y, learning_rate=0.01, epochs=1000, tolerance=1e-6):\n",
    "    n = len(x)\n",
    "    b0 = 0\n",
    "    b1 = 0\n",
    "    prev_loss = float('inf')\n",
    "\n",
    "    for i in range(epochs):\n",
    "        y_pred = b0 + b1*x\n",
    "\n",
    "        gradient_b0 = (-2/n) * np.sum(y - y_pred)\n",
    "        gradient_b1 = (-2/n) * np.sum((y - y_pred) * x)\n",
    "\n",
    "        b0 -= learning_rate * gradient_b0\n",
    "        b1 -= learning_rate * gradient_b1\n",
    "\n",
    "        loss = np.sum((y - y_pred)**2)\n",
    "\n",
    "        if abs(prev_loss - loss) < tolerance:\n",
    "            break\n",
    "\n",
    "        prev_loss = loss\n",
    "\n",
    "    return b0, b1, loss\n",
    "\n",
    "b0_f, b1_f, sse_f = full_batch_gradient_descent(x, y)\n",
    "print(\"\\nFull-batch Gradient Descent:\")\n",
    "print(\"b0:\",b0_f)\n",
    "print(\"b1:\",b1_f)\n",
    "print(\"SSE:\",sse_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9fad40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression using stochastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "560889a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stochastic Gradient Descent:\n",
      "b0 1.2329276979312107\n",
      "b1: 1.2938900338635813\n",
      "SSE: 9.981772104293816\n"
     ]
    }
   ],
   "source": [
    "def stochastic_gradient_descent(x, y, learning_rate=0.01, epochs=1000, tolerance=1e-6):\n",
    "    n = len(x)\n",
    "    b0 = 0\n",
    "    b1 = 0\n",
    "    prev_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        index = np.random.permutation(n)\n",
    "        x_shuffled = x[index]\n",
    "        y_shuffled = y[index]\n",
    "\n",
    "        for i in range(n):\n",
    "            y_pred = b0 + b1*x_shuffled[i]\n",
    "\n",
    "            gradient_b0 = -2 * (y_shuffled[i] - y_pred)\n",
    "            gradient_b1 = -2 * (y_shuffled[i] - y_pred) * x_shuffled[i]\n",
    "\n",
    "            b0 -= learning_rate * gradient_b0\n",
    "            b1 -= learning_rate * gradient_b1\n",
    "\n",
    "        y_pred = b0 + b1*x\n",
    "        loss = np.sum((y - y_pred)**2)\n",
    "\n",
    "        if abs(prev_loss - loss) < tolerance:\n",
    "            break\n",
    "\n",
    "        prev_loss = loss\n",
    "\n",
    "    return b0, b1, loss\n",
    "\n",
    "\n",
    "b0_s, b1_s, sse_s = stochastic_gradient_descent(x, y)\n",
    "print(\"\\nStochastic Gradient Descent:\")\n",
    "print(\"b0\",b0_s)\n",
    "print(\"b1:\",b1_s)\n",
    "print(\"SSE:\",sse_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54a03ac",
   "metadata": {},
   "source": [
    "# 2. Download California Housing Rate Dataset. Analyse the input attributes and find out the attribute that best follow the linear relationship with the output price. Implement both the analytic formulation and gradient descent (Full-batch, stochastic) on LMS loss formulation to compute the coefficients of regression matrix and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b06f0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients using Analytic Formulation: [44459.72916908 41933.84939381]\n",
      "Coefficients using Full-batch Gradient Descent: [39148.47787113 43047.96802282]\n",
      "Coefficients using Stochastic Gradient Descent: [45441.62996896 42427.8630104 ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing_data = pd.read_csv(\"D:/sem-6/ML-Lab/week-4/california housing.csv\")\n",
    "\n",
    "selected_attribute = 'median_income'\n",
    "X = housing_data[selected_attribute].values.reshape(-1, 1)\n",
    "y = housing_data['median_house_value'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_with_intercept = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "X_test_with_intercept = np.c_[np.ones(X_test.shape[0]), X_test]\n",
    "\n",
    "theta_analytic = np.linalg.inv(X_train_with_intercept.T.dot(X_train_with_intercept)).dot(X_train_with_intercept.T).dot(y_train)\n",
    "print(\"Coefficients using Analytic Formulation:\", theta_analytic)\n",
    "\n",
    "def full_batch_gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    for _ in range(num_iterations):\n",
    "        y_pred = X.dot(theta)\n",
    "        theta -= (1/len(y)) * learning_rate * X.T.dot(y_pred - y)\n",
    "    return theta\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "theta_full_batch = full_batch_gradient_descent(X_train_with_intercept, y_train, learning_rate, num_iterations)\n",
    "print(\"Coefficients using Full-batch Gradient Descent:\", theta_full_batch)\n",
    "\n",
    "def stochastic_gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    for _ in range(num_iterations):\n",
    "        for i in range(len(y)):\n",
    "            rand_index = np.random.randint(0, len(y))\n",
    "            xi = X[rand_index]\n",
    "            yi = y[rand_index]\n",
    "            y_pred = np.dot(xi, theta)\n",
    "            theta -= learning_rate * xi * (y_pred - yi)\n",
    "    return theta\n",
    "\n",
    "theta_stochastic = stochastic_gradient_descent(X_train_with_intercept, y_train, learning_rate, num_iterations)\n",
    "print(\"Coefficients using Stochastic Gradient Descent:\", theta_stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa779993",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_analytic = X_test_with_intercept.dot(theta_analytic)\n",
    "y_pred_full_batch = X_test_with_intercept.dot(theta_full_batch)\n",
    "y_pred_stochastic = X_test_with_intercept.dot(theta_stochastic)\n",
    "\n",
    "SSE_analytic = np.sum((y - y_pred_analytic) ** 2)\n",
    "SSE_full_batch = np.sum((y - y_pred_full_batch) ** 2)\n",
    "SSE_stochastic = np.sum((y - y_pred_stochastic) ** 2)\n",
    "\n",
    "mean_y = np.mean(y)\n",
    "SST = np.sum((y - mean_y) ** 2)\n",
    "\n",
    "R_squared_analytic = 1 - (SSE_analytic / SST)\n",
    "R_squared_full_batch = 1 - (SSE_full_batch / SST)\n",
    "R_squared_stochastic = 1 - (SSE_stochastic / SST)\n",
    "\n",
    "print(\"SSE and R-squared value:\")\n",
    "print(\"Analytic Formulation: SSE =\", SSE_analytic, \", R-squared =\", R_squared_analytic)\n",
    "print(\"Full-batch Gradient Descent: SSE =\", SSE_full_batch, \", R-squared =\", R_squared_full_batch)\n",
    "print(\"Stochastic Gradient Descent: SSE =\", SSE_stochastic, \", R-squared =\", R_squared_stochastic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
